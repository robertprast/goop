[{"id":"goop","user_id":"ebcc9212-c16c-42be-842c-043ec09cecbc","name":"GOOP","type":"pipe","content":"from typing import Union, Iterator\nimport boto3\nimport vertexai\nfrom vertexai.preview.generative_models import GenerativeModel\nfrom openai import OpenAI, AzureOpenAI\n\n\nclass Pipe:\n    def __init__(self):\n        self.type = \"manifold\"\n        self.openai_client = OpenAI(\n            base_url=\"http://host.docker.internal:8080/openai/v1\",\n            api_key=\"test\",\n        )\n        self.azure_client = AzureOpenAI(\n            base_url=\"http://host.docker.internal:8080/azure\",\n            api_key=\"test\",\n            api_version=\"test\",\n        )\n        self.bedrock_client = boto3.client(\n            \"bedrock-runtime\",\n            endpoint_url=\"http://host.docker.internal:8080/bedrock\",\n            region_name=\"us-east-1\",\n            aws_access_key_id=\"test\",\n            aws_secret_access_key=\"test\",\n        )\n\n        from google.auth.credentials import AnonymousCredentials\n\n        class DummyCredentials(AnonymousCredentials):\n            pass\n\n        self.vertex_endpoint = \"http://host.docker.internal:8080/vertex\"\n        self.vertex_project_id = \"encrypted-llm\"\n        vertexai.init(\n            project=self.vertex_project_id,\n            api_endpoint=self.vertex_endpoint,\n            credentials=DummyCredentials(),\n            api_transport=\"rest\",\n        )\n\n    def pipes(self):\n        return [\n            ##################\n            # OPENAI MODELS  #\n            ##################\n            {\n                \"id\": \"openai/gpt-4o\",\n                \"name\": \"openai/gpt-4o\",\n            },\n            {\n                \"id\": \"openai/gpt-4o-mini\",\n                \"name\": \"openai/gpt-4o-mini\",\n            },\n            # {\n            #     \"id\": \"azure/gpt-4o\",\n            #     \"name\": \"azure/gpt-4o\",\n            # },\n            ##################\n            # BEDROCK MODELS #\n            ##################\n            {\n                \"id\": \"bedrock/us.anthropic.claude-3-haiku-20240307-v1:0\",\n                \"name\": \"bedrock/claude-3-haiku\",\n            },\n            {\n                \"id\": \"bedrock/us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n                \"name\": \"bedrock/claude-3-5-sonnet\",\n            },\n            {\n                \"id\": \"bedrock/us.meta.llama3-2-11b-instruct-v1:0\",\n                \"name\": \"bedrock/llama3.2-11b\",\n            },\n            {\n                \"id\": \"bedrock/us.meta.llama3-2-1b-instruct-v1:0\",\n                \"name\": \"bedrock/llama3.2-1b\",\n            },\n            {\n                \"id\": \"bedrock/us.meta.llama3-2-3b-instruct-v1:0\",\n                \"name\": \"bedrock/llama3.2-3b\",\n            },\n            {\n                \"id\": \"bedrock/us.meta.llama3-2-90b-instruct-v1:0\",\n                \"name\": \"bedrock/llama3.2-90b\",\n            },\n            # {\n            #     \"id\": \"bedrock/us.anthropic.claude-3-opus-20240229-v1:0\",\n            #     \"name\": \"bedrock/claude-3-opus\",\n            # },\n            # {\n            #     \"id\": \"bedrock/us.anthropic.claude-3-sonnet-20240229-v1:0\",\n            #     \"name\": \"bedrock/claude-3-sonnet\",\n            # },\n            ##################\n            # VERTEX MODELS  #\n            ##################\n            {\n                \"id\": \"vertex/gemini-1.5-flash-002\",\n                \"name\": \"vertex/gemini-1.5-flash\",\n            },\n            {\n                \"id\": \"vertex/gemini-1.5-pro-002\",\n                \"name\": \"vertex/gemini-1.5-pro\",\n            },\n        ]\n\n    def pipe(self, body: dict, __user__: dict) -> Union[str, Iterator]:\n        model_name = body[\"model\"].replace(\"openai_proxy_pipe.\", \"\")\n        if model_name.startswith(\"openai/\"):\n            body[\"model\"] = model_name.replace(\"openai/\", \"\")\n            return self._handle_openai(body)\n\n        elif model_name.startswith(\"azure/\"):\n            body[\"model\"] = model_name.replace(\"azure/\", \"\")\n            return self._handle_azure(body)\n        elif model_name.startswith(\"bedrock/\"):\n            body[\"model\"] = model_name.replace(\"bedrock/\", \"\")\n            return self._handle_bedrock(body)\n        elif model_name.startswith(\"vertex/\"):\n            body[\"model\"] = model_name.replace(\"vertex/\", \"\")\n            return self._handle_vertex(body)\n        else:\n            return f\"Error: Unsupported model prefix for {model_name}\"\n\n    def _handle_openai(self, body: dict):\n        model_id = body[\"model\"]\n        payload = {**body, \"model\": model_id}\n        try:\n            response = self.openai_client.chat.completions.create(**payload)\n            if body.get(\"stream\", False):\n\n                def stream_generator():\n                    for i in response:\n                        if len(i.choices) > 0 and i.choices[0].delta.content:\n                            yield i.choices[0].delta.content\n\n                return stream_generator()\n            else:\n                return response.choices[0].message.content\n        except Exception as e:\n            return f\"Error: {e}\"\n\n    def _handle_azure(self, body: dict):\n        model_id = body[\"model\"]\n        payload = {**body, \"model\": model_id}\n        try:\n            response = self.azure_client.chat.completions.create(**payload)\n            if body.get(\"stream\", False):\n\n                def stream_generator():\n                    for i in response:\n                        if len(i.choices) > 0 and i.choices[0].delta.content:\n                            yield i.choices[0].delta.content\n\n                return stream_generator()\n            else:\n                return response.choices[0].message.content\n        except Exception as e:\n            return f\"Error: {e}\"\n\n    def _handle_bedrock(self, body: dict):\n        model_id = body[\"model\"]\n        try:\n\n            def _replace_headers(request, **kwargs):\n                request.headers[\"Authorization\"] = \"Bearer test\"\n\n            self.bedrock_client.meta.events.register(\n                \"before-send.bedrock-runtime.*\", _replace_headers\n            )\n            conversation = []\n            for message in body.get(\"messages\", []):\n                conversation.append(\n                    {\n                        \"role\": message[\"role\"],\n                        \"content\": [{\"text\": message[\"content\"]}],\n                    }\n                )\n            if body.get(\"stream\", False):\n\n                def stream_generator():\n                    streaming_response = self.bedrock_client.converse_stream(\n                        modelId=model_id,\n                        messages=conversation,\n                    )\n\n                    for chunk in streaming_response[\"stream\"]:\n                        if \"contentBlockDelta\" in chunk:\n                            text = chunk[\"contentBlockDelta\"][\"delta\"][\"text\"]\n                            if text:\n                                yield text\n\n                return stream_generator()\n\n            response = self.bedrock_client.converse(\n                modelId=model_id,\n                messages=conversation,\n            )\n            response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n            if response_text:\n                return response_text\n            else:\n                raise Exception(\"No response text found in the response.\")\n        except Exception as e:\n            return f\"Error: {e}\"\n\n    def _handle_vertex(self, body: dict):\n        model_id = body[\"model\"]\n        try:\n            generative_model = GenerativeModel(model_id)\n            contents = [\n                {\"role\": message[\"role\"], \"parts\": [{\"text\": message[\"content\"]}]}\n                for message in body.get(\"messages\", [])\n            ]\n\n            if body.get(\"stream\", False):\n\n                def stream_generator():\n                    # TODO - Implement streaming for VertexAI, this is a blocking call for now\n                    response = generative_model.generate_content(\n                        contents,\n                        generation_config=body.get(\"generation_config\", {}),\n                        safety_settings=body.get(\"safety_settings\", {}),\n                        stream=True,\n                    )\n                    for chunk in response:\n                        if (\n                            len(chunk.candidates) > 0\n                            and chunk.candidates[0].content.parts[0].text\n                        ):\n                            yield chunk.candidates[0].content.parts[0].text\n\n                return stream_generator()\n            else:\n                response = generative_model.generate_content(\n                    contents,\n                    generation_config=body.get(\"generation_config\", {}),\n                    safety_settings=body.get(\"safety_settings\", {}),\n                )\n                return response.candidates[0].content.parts[0].text\n        except Exception as e:\n            return f\"Error: {e}\"\n","meta":{"description":"All LLM Cloud Providers in a Single Reverse Proxy","manifest":{}},"is_active":true,"is_global":false,"updated_at":1729407532,"created_at":1729407519}]